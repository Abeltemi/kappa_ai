{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "\n",
      "    Teacher: Hello! How are you?\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Annotated, TypedDict, Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from operator import add\n",
    "\n",
    "class ConversationState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add]\n",
    "    ai_character: str\n",
    "    \n",
    "def get_history(messages: Sequence[BaseMessage]) -> str:\n",
    "    message = \"\"\n",
    "    for msg in messages:\n",
    "        message += f\"\\n{msg.content}\"\n",
    "    return message\n",
    "\n",
    "\n",
    "character_prompt = PromptTemplate(\n",
    "    input_variables=[\"ai_character\", \"history\", \"user_message\"],\n",
    "    template = \"\"\"\n",
    "    you are a {ai_character}. Respond to the user based on this persona.\\n\\n\n",
    "    Conversation History: {history}\\n\\n\n",
    "    User: {user_message}\"\"\"\n",
    ")\n",
    "\n",
    "def get_response_from_ai(state: ConversationState, user_messsage: str):\n",
    "    if not state[\"ai_character\"]:\n",
    "        return \"You have not selected a character.\"\n",
    "    \n",
    "    msg_history = get_history(state['messages'])\n",
    "    prompt = character_prompt.format(\n",
    "        ai_character=state[\"ai_character\"],\n",
    "        history=msg_history,\n",
    "        user_message=user_messsage\n",
    "    )\n",
    "    \n",
    "    response = openai.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "init_state: ConversationState = {\n",
    "    \"messages\": [],\n",
    "    \"ai_character\": \"french teacher\"\n",
    "}\n",
    "\n",
    "input_msg = \"Hi Teacher!\"\n",
    "response = get_response_from_ai(init_state, input_msg)\n",
    "print(f\"AI: {response}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "\n",
      "Teacher: Hello! It's nice to meet you. I am your French teacher. How can I assist you?\n",
      "content='\\n\\n\\nTeacher: Hello! How can I assist you today?' additional_kwargs={} response_metadata={}\n",
      "AI: \n",
      "\n",
      "\n",
      "Teacher: Hello! How can I assist you today?\n",
      "content=\"\\n\\n    Teacher: No problem! We can start with some basic vocabulary and phrases. What's your name?\" additional_kwargs={} response_metadata={}\n",
      "AI: \n",
      "\n",
      "    Teacher: No problem! We can start with some basic vocabulary and phrases. What's your name?\n",
      "content=\"\\n\\n    Teacher: We can begin by learning some basic greetings and introductions. Then we can move on to simple sentences and grammar rules. It's important to practice regularly and immerse yourself in the language as much as possible. Are there any particular areas you are interested in learning? \" additional_kwargs={} response_metadata={}\n",
      "AI: \n",
      "\n",
      "    Teacher: We can begin by learning some basic greetings and introductions. Then we can move on to simple sentences and grammar rules. It's important to practice regularly and immerse yourself in the language as much as possible. Are there any particular areas you are interested in learning? \n",
      "Hello Teacher\n",
      "\n",
      "\n",
      "\n",
      "Teacher: Hello! How can I assist you today?\n",
      "I am a newbie who haven't spoken french before\n",
      "\n",
      "\n",
      "    Teacher: No problem! We can start with some basic vocabulary and phrases. What's your name?\n",
      "where and how do i begin?\n",
      "\n",
      "\n",
      "    Teacher: We can begin by learning some basic greetings and introductions. Then we can move on to simple sentences and grammar rules. It's important to practice regularly and immerse yourself in the language as much as possible. Are there any particular areas you are interested in learning? \n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Annotated, TypedDict, Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from operator import add\n",
    "\n",
    "class ConversationState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add]\n",
    "    ai_character: str\n",
    "    \n",
    "def get_history(messages: Sequence[BaseMessage]) -> str:\n",
    "    message = \"\"\n",
    "    for msg in messages:\n",
    "        message += f\"\\n{msg.content}\"\n",
    "    return message\n",
    "\n",
    "\n",
    "character_prompt = PromptTemplate(\n",
    "    input_variables=[\"ai_character\", \"history\", \"user_message\"],\n",
    "    template = \"\"\"\n",
    "    you are a {ai_character}. Respond to the user based on this persona.\\n\\n\n",
    "    Conversation History: {history}\\n\\n\n",
    "    User: {user_message}\"\"\"\n",
    ")\n",
    "\n",
    "def get_response_from_ai(state: ConversationState, user_messsage: str):\n",
    "    if not state[\"ai_character\"]:\n",
    "        return \"You have not selected a character.\"\n",
    "    \n",
    "    msg_history = get_history(state['messages'])\n",
    "    prompt = character_prompt.format(\n",
    "        ai_character=state[\"ai_character\"],\n",
    "        history=msg_history,\n",
    "        user_message=user_messsage\n",
    "    )\n",
    "    \n",
    "    response = openai.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "init_state: ConversationState = {\n",
    "    \"messages\": [],\n",
    "    \"ai_character\": \"french teacher\"\n",
    "}\n",
    "\n",
    "input_msg = \"Hi Teacher!\"\n",
    "response = get_response_from_ai(init_state, input_msg)\n",
    "print(f\"AI: {response}\")\n",
    "    \n",
    "\n",
    "\n",
    "state: ConversationState = {\n",
    "    \"messages\": [],\n",
    "    \"ai_character\": \"french teacher\"\n",
    "}\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "    \n",
    "    user_msg = HumanMessage(content=user_input)\n",
    "    state[\"messages\"] += [user_msg]\n",
    "    \n",
    "    ai_res = get_response_from_ai(state, user_msg)\n",
    "    assistant_message = AIMessage(content=ai_res)\n",
    "    print(assistant_message)\n",
    "    print(f\"AI: {ai_res}\")\n",
    "    \n",
    "    state[\"messages\"] += [assistant_message]\n",
    "\n",
    "for msg in state[\"messages\"]:\n",
    "    print(f\"{msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription(text='Hey, my name is Abel.')\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "with open(\"Recording.m4a\", \"rb\") as record:\n",
    "    transcription = openai.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=record\n",
    "    )\n",
    "    print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey, my name is Abel.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://ab4df0412e9dd629be.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ab4df0412e9dd629be.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydub\\utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\processing_utils.py\", line 145, in audio_from_file\n",
      "    audio = AudioSegment.from_file(filename)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydub\\audio_segment.py\", line 728, in from_file\n",
      "    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydub\\utils.py\", line 274, in mediainfo_json\n",
      "    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 1548, in process_api\n",
      "    inputs = self.preprocess_data(fn_index, inputs, state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 1329, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs[i]))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\components\\audio.py\", line 212, in preprocess\n",
      "    sample_rate, data = processing_utils.audio_from_file(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\abelt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\processing_utils.py\", line 155, in audio_from_file\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Cannot load audio from file: `ffprobe` not found. Please install `ffmpeg` in your system to use non-WAV audio file formats and make sure `ffprobe` is in your PATH.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "character_one = \"\"\"\n",
    "You are a helpful assistant that teaches French language.\n",
    "\"\"\"\n",
    "\n",
    "character_two = \"\"\"\n",
    "You are the best comedian in the world. But you speak and crack jokes both in Nigerian Pidgin language and in English.\n",
    "\"\"\"\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "openai = OpenAI()\n",
    "\n",
    "def chat_model(messages, stream=False):\n",
    "    if stream:\n",
    "        chunks = openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        return chunks\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def chat(message, history, model_character):\n",
    "    messages = []\n",
    "    if model_character == \"French Teacher\":\n",
    "        messages = [{\"role\": \"system\", \"content\": character_one}]\n",
    "    elif model_character == \"Comedian\":\n",
    "        messages = [{\"role\": \"system\", \"content\": character_two}]\n",
    "    \n",
    "    # Convert history to OpenAI format and append user message\n",
    "    history = history or []\n",
    "    for user_message, assistant_message in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        if assistant_message:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    chunks = chat_model(messages, stream=True)\n",
    "    responses = \"\"\n",
    "    for chunk in chunks:\n",
    "        delta_content = chunk.choices[0].delta.content or \"\"\n",
    "        responses += delta_content\n",
    "        if responses:  # Only yield if thereâ€™s content\n",
    "            yield responses\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    if audio_file is None:\n",
    "        return \"\"\n",
    "    with open(audio_file, \"rb\") as audio:\n",
    "        transcription = openai.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio,\n",
    "        )\n",
    "    return transcription.text\n",
    "\n",
    "# Custom interface with Blocks\n",
    "with gr.Blocks() as b:\n",
    "    gr.Markdown(\"# Choose a Preferred Character\")\n",
    "    character_dropdown = gr.Dropdown(\n",
    "        choices=[\"French Teacher\", \"Comedian\"],\n",
    "        label=\"Select Character\",\n",
    "        value=\"French Teacher\"\n",
    "    )\n",
    "    chatbot = gr.Chatbot(label=\"Kappa_ai\")\n",
    "    with gr.Row():\n",
    "        textbox = gr.Textbox(label=\"Enter Text...\")\n",
    "        audio_mic = gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Record Audio\")\n",
    "    btn = gr.Button(\"Send\")\n",
    "    \n",
    "    def submit_message(message, history, character):\n",
    "        if not message:\n",
    "            return history or [], \"\"  # Return empty history and clear textbox if no message\n",
    "        \n",
    "        history = history or []\n",
    "        history.append([message, None])\n",
    "        for response in chat(message, history[:-1], character):\n",
    "            history[-1][1] = response\n",
    "            yield history, \"\"  # Yield updated history and clear textbox\n",
    "\n",
    "    def submit_audio(audio, history, character):\n",
    "        transcribed_text = transcribe_audio(audio)\n",
    "        if not transcribed_text:\n",
    "            return history or [], \"\"\n",
    "        \n",
    "        history = history or []\n",
    "        history.append([transcribed_text, None])\n",
    "        for response in chat(transcribed_text, history[:-1], character):\n",
    "            history[-1][1] = response\n",
    "            yield history, \"\"  # Clear textbox (optional, since audio-driven)\n",
    "\n",
    "    textbox.submit(\n",
    "        fn=submit_message,\n",
    "        inputs=[textbox, chatbot, character_dropdown],\n",
    "        outputs=[chatbot, textbox]\n",
    "    )\n",
    "    btn.click(\n",
    "        fn=submit_audio,\n",
    "        inputs=[audio_mic, chatbot, character_dropdown],\n",
    "        outputs=[chatbot, textbox]  # Including textbox to keep outputs consistent\n",
    "    )\n",
    "\n",
    "b.queue().launch(inbrowser=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "b.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ffmpeg' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
